{"cells":[{"cell_type":"markdown","metadata":{"id":"klgcYJegaGqQ"},"source":["# Optional: Batch Normalization & Dropout \n","\n","In this Notebook we will introduce the idea of Batch Normalization and Dropout and how both these methods help in Neural Network training. \n","\n","By completing this exercise you will:\n","1. Know the implementation details of Batch Norm and Dropout\n","2. Notice the difference in behaviour during train and test time\n","3. Use Batch Norm and Dropout in a Fully Connected Layer to see how it affects training\n","\n","Let us start with Batch Normalization:"]},{"cell_type":"markdown","metadata":{"id":"fvoISdm0aGqR"},"source":["# 1. Batch Normalization\n","\n","## 1.1 What is Batch Normalization\n","One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization which was proposed by [1].\n","\n","The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n","\n","The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [1] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.\n","\n","It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.\n","\n","[1] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n","Internal Covariate Shift\", ICML 2015."]},{"cell_type":"markdown","metadata":{"id":"ybxyQjUoaGqS"},"source":["### Before We Start\n","\n","It is important that we take a look at the Mathematical formula behind Batch Norm. Please make sure to understand the formula below since this will definitely help you with the implementation later. :)"]},{"cell_type":"markdown","metadata":{"id":"Jzr34XgIaGqS"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"R7EiCCn-aGqS"},"source":["### A quick explanation of the formula\n","It may look a bit confusing at the first glance, but that's not true. Let's summarize the mathematics here: In the left column, we are given the input data, which consists (as always) of $N$ samples of dimension $D$. Furthermore, we need the learnable shift and scale parameters which we call $\\beta$ and $\\gamma$. The intermediates are describing the mean $\\mu$ and variance $\\sigma$ that we need to compute from the input data and then $\\hat{x}$ which is the normalized input data. The output is given by $y$ which is the combination of the normalized data with the learnable parameters. \n","\n","The right column contains the mathematical formulations and can be summarized as follows:\n","1. For the given input x, we calculate the mean $\\mu$ across all input samples.\n","2. Based on the mean $\\mu$, we compute the variances $\\sigma$ of each value in the sample.\n","3. We then normalize the input data based on the computed mean and variance.\n","4. Finally, we combine the normalized data with the learnable parameters $\\gamma$ and $\\beta$.\n","\n","Please remember that Batch Normalization behaves differently at training and test time. In the figure above, we see the behaviour at training time. "]},{"cell_type":"markdown","metadata":{"id":"pSEl5NrQaGqS"},"source":["## 1.2 Batch Normalization: Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UFsqFAKaGqT"},"outputs":[],"source":["# As usual, a bit of setup\n","\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from exercise_code.layers import *\n","from exercise_code.tests import *\n","import torch.nn as nn\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import pytorch_lightning as pl\n","import os\n","import shutil\n","from pytorch_lightning.loggers import TensorBoardLogger\n","\n","from exercise_code.BatchNormModel import SimpleNetwork, BatchNormNetwork, DropoutNetwork\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","# supress cluttering warnings in solutions\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","metadata":{"id":"_48FcGcMaGqT"},"source":["### Batch normalization: Forward Pass\n","\n","<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>In the file <code>exercise_code/layers.py </code>, we have implemented the <code>batchnorm_forward</code> function. Read this implementation and make sure you understand what batch normalization is doing. Then execute the following cells to test the implementation.\n"," </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiaZQgB5aGqT"},"outputs":[],"source":["# Check the training-time forward pass by checking means and variances\n","# of features both before and after batch normalization\n","\n","# Simulate the forward pass for a two-layer network\n","N, D1, D2, D3 = 200, 50, 60, 3\n","X = np.random.randn(N, D1)\n","W1 = np.random.randn(D1, D2)\n","W2 = np.random.randn(D2, D3)\n","a = np.maximum(0, X.dot(W1)).dot(W2)\n","\n","print('Before batch normalization:')\n","print('  means: ', a.mean(axis=0))\n","print('  stds: ', a.std(axis=0), '\\n')\n","\n","# Means should be close to zero and stds close to one\n","print('After batch normalization with (gamma=1, beta=0)')\n","a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n","print('  mean: ', a_norm.mean(axis=0))\n","print('  std: ', a_norm.std(axis=0) , '\\n')\n","\n","# Now means should be close to beta and stds close to gamma\n","gamma = np.asarray([1.0, 2.0, 3.0])\n","beta = np.asarray([11.0, 12.0, 13.0])\n","a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n","print('After batch normalization with (nontrivial gamma, beta)')\n","print('  means: ', a_norm.mean(axis=0))\n","print('  stds: ', a_norm.std(axis=0) )"]},{"cell_type":"markdown","metadata":{"id":"YoVBI4fuaGqU"},"source":["Since the mean and variances in batch norm are computed in training time,\n","before invoking the test-time forward pass run the training-time\n","forward pass (previous cell) many times to warm up the running averages. Then\n","checking the means and variances of activations for a test-time\n","forward pass."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThH_gAUSaGqU"},"outputs":[],"source":["# Check the test-time forward pass by checking means and variances \n","# of features after batch normalization\n","\n","N, D1, D2, D3 = 200, 50, 60, 3\n","W1 = np.random.randn(D1, D2)\n","W2 = np.random.randn(D2, D3)\n","\n","bn_param = {'mode': 'train'}\n","gamma = np.ones(D3)\n","beta = np.zeros(D3)\n","for t in range(50):\n","    X = np.random.randn(N, D1)\n","    a = np.maximum(0, X.dot(W1)).dot(W2)\n","    batchnorm_forward(a, gamma, beta, bn_param)\n","bn_param['mode'] = 'test'\n","X = np.random.randn(N, D1)\n","a = np.maximum(0, X.dot(W1)).dot(W2)\n","a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n","\n","# Means should be close to zero and stds close to one, but will be\n","# noisier than training-time forward passes.\n","print('After batch normalization (test-time):')\n","print('  means: ', a_norm.mean(axis=0))\n","print('  stds: ', a_norm.std(axis=0))"]},{"cell_type":"markdown","metadata":{"id":"Z4HNajRVaGqU"},"source":["### Batch Normalization: Backward Pass\n","Since batch normalization is realized by a more complex function of learnable parameters, it is a good exercise to train your backprop skills through this computational graph.\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Open <code>exercise_code/layer.py</code> and implement the backward pass for Batch Normalization in the function <code> batchnorm_backward() </code>.\n","    </p>\n","    <p> To derive the backward pass you should write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; make sure to sum gradients across these branches in the backward pass. You can stay close to the forward pass implementation we have provided for you, i.e. go line by line backward.\n","    </p>\n","    <p> Once you have finished, run the following to numerically check your backward pass.\n","    </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQ8XjsiWaGqU"},"outputs":[],"source":["# Gradient check batchnorm backward pass\n","\n","N, D = 4, 5\n","x = 5 * np.random.randn(N, D) + 12\n","gamma = np.random.randn(D)\n","beta = np.random.randn(D)\n","dout = np.random.randn(N, D)\n","\n","bn_param = {'mode': 'train'}\n","\n","fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n","fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n","fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","da_num = eval_numerical_gradient_array(fg, gamma, dout)\n","db_num = eval_numerical_gradient_array(fb, beta, dout)\n","\n","_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n","dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dgamma error: ', rel_error(da_num, dgamma))\n","print('dbeta error: ', rel_error(db_num, dbeta))"]},{"cell_type":"markdown","metadata":{"id":"93Xbj-yfaGqV"},"source":["## 1.3 Using Batch Normalization with PyTorch Lightning\n","\n","Now that we have seen the implementation of Batch Normalization, it is interesting to see how it would affect the overall Model Performance. Since you have already worked with PyTorch Lightning in the last exercise, you have seen how easy it makes our lives. As an experiment, we will use a simple Fully Connected Network in PyTorch Lightning here."]},{"cell_type":"markdown","metadata":{"id":"kKU85ZpKaGqV"},"source":["### Setup TensorBoard\n","In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations your TensorBoard!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sahveAH7aGqV"},"outputs":[],"source":["# Few Hyperparameters before we start things off\n","hidden_dim = 200\n","batch_size = 50\n","\n","logdir = './batch_norm_logs'\n","if os.path.exists(logdir):\n","    # We delete the logs on the first run\n","    shutil.rmtree(logdir)\n","os.mkdir(logdir)\n","\n","epochs = 5\n","learning_rate = 0.00005"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hNWcm6laGqV"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir batch_norm_logs"]},{"cell_type":"markdown","metadata":{"id":"kjPgHkQ_aGqV"},"source":["### Train a model without Batch Normalization. \n","\n","<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>Let us first start with a simple network which does not make use of Batch Normalization. We have already implemented the a simple network <code>SimpleNetwork</code> in <code>exercise_code/BatchNormModel.py</code>. Feel free to check it out and play around with the parameters. The cell below is setting up a short trainings process for this network.\n"," </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4NXtCGraGqW"},"outputs":[],"source":["# train\n","model = SimpleNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate)\n","# Creating a logging object\n","simple_network_logger = TensorBoardLogger(\n","    save_dir=logdir,\n","    name='simple_network'\n",")\n","trainer = pl.Trainer(max_epochs=epochs, logger=simple_network_logger)\n","\n","trainer.fit(model)"]},{"cell_type":"markdown","metadata":{"id":"jmozk_SGaGqW"},"source":["### Train a model incl. Batch Normalization\n","\n","<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p> Now that we have already seen how our simple network should work, let us look at a model that is actually using Batch Normalization. Again, we provide you with such a model <code>BatchNormNetwork</code> in <code>exercise_code/BatchNormModel.py</code>. Same as before: Feel free to check it out and play around with the parameters. The cell below is setting up a short trainings process for this model. \n"," </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nVnuM9caGqW"},"outputs":[],"source":["model = BatchNormNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate)\n","batch_norm_network_logger = TensorBoardLogger(\n","    save_dir=logdir,\n","    name='batch_norm_network'\n",")\n","trainer = pl.Trainer(max_epochs=epochs, logger=batch_norm_network_logger)\n","trainer.fit(model)"]},{"cell_type":"markdown","metadata":{"id":"I0QWU5oBaGqW"},"source":["### Observations\n","Take a look at TensorBoard to compare the performance of both networks:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nVbNzUEaGqW"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir batch_norm_logs"]},{"cell_type":"markdown","metadata":{"id":"75HcxQOFaGqX"},"source":["As you can see, using Batch Normalization resulted in better performance. You can easily observe lower validation loss and higher validation accuracy from the graphs. Batch Norm in general is helpful since it would lead to faster model training.\n","\n","Batch Normalization has other related benefits, for instance, it provides a bit of regularization. However, we would look for better methods of regularization such as Dropout. So in the second part of this notebook, let's have a more detailed look at the effect of Dropout. :)"]},{"cell_type":"markdown","metadata":{"id":"Kxval8YaaGqX"},"source":["# 2. Dropout"]},{"cell_type":"markdown","metadata":{"id":"JAY5pOtCaGqX"},"source":["## 2.1 What is Dropout\n","\n","Dropout [1] is a technique for regularizing neural networks by randomly setting some features to zero during the forward pass. While training, dropout is implemented by only keeping a neuron active with some probability p\n","(a hyperparameter), or setting it to zero otherwise. The Dropout technique would help your Neural Network to perform better on Test data.\n","\n","We want to repeat the approach that we saw above for Batch Normalization, but this time for Dropout. Let us thus first have a look at the implementation and then compare two networks with each other where one is using Dropout and one is not. \n","\n","[1] Srivastava et al, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", 2014"]},{"cell_type":"markdown","metadata":{"id":"hxZ1wZIraGqX"},"source":["<img src=\"./img/dropout.jpg\">"]},{"cell_type":"markdown","metadata":{"id":"FmTbh8xraGqX"},"source":["## 2.2 Dropout Implementation"]},{"cell_type":"markdown","metadata":{"id":"7TvhkmVNaGqX"},"source":["### Dropout: Forward Pass\n","\n","The dropout method is a little less complex to implement than the Batch Normalization, hence we ask you to implement both, the forward and the backward pass. Let us start with the forward pass:\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p> In the file <code>exercise_code/layers.py</code>, implement the forward pass for Dropout in <code>dropout_forward()</code>. Since Dropout behaves differently during training and testing, make sure to implement the operation for both modes.\n","    </p>\n","    <p> Once you have done so, run the cell below to test your implementation.\n","    </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJaCzHHkaGqX"},"outputs":[],"source":["x = np.random.randn(500, 500) + 10\n","# Let us use different dropout values(p) for our dropout layer and see their effects\n","for p in [0.3, 0.6, 0.75]:\n","    out, _ = dropout_forward(x, {'mode': 'train', 'p': p})\n","    out_test, _ = dropout_forward(x, {'mode': 'test', 'p': p})\n","\n","    print('Running tests with p = ', p)\n","    print('Mean of input: ', x.mean())\n","    print('Mean of train-time output: ', out.mean())\n","    print('Mean of test-time output: ', out_test.mean())\n","    print('Fraction of train-time output set to zero: ', (out == 0).mean())\n","    print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"ysbFGTvTaGqY"},"source":["### Dropout: Backward Pass\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p> In the file <code>exercise_code/layers.py</code>, implement the backward pass for dropout in <code>dropout_backward()</code>. After doing so, run the following cell to numerically gradient-check your implementation.\n","    </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZguaoPL8aGqY"},"outputs":[],"source":["x = np.random.randn(10, 10) + 10\n","dout = np.random.randn(*x.shape)\n","\n","dropout_param = {'mode': 'train', 'p': 0.8, 'seed': 123}\n","out, cache = dropout_forward(x, dropout_param)\n","dx = dropout_backward(dout, cache)\n","dx_num = eval_numerical_gradient_array(lambda xx: dropout_forward(xx, dropout_param)[0], x, dout)\n","\n","print('dx relative error: ', rel_error(dx, dx_num))"]},{"cell_type":"markdown","metadata":{"id":"zToubtfPaGqY"},"source":["## 2.3 Using Dropout with PyTorch Lightning\n","\n","Same experiment as for Batch Normalization: We will train a pair of two-layer networks on a training dataset where one network will use no Dropout and one will use a Dropout probability of 0.75. We will then visualize the training and validation accuracies of the two networks over time."]},{"cell_type":"markdown","metadata":{"id":"2IWYwQ72aGqY"},"source":["### Setup TensorBoard\n","\n","In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations to your TensorBoard!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjvGjIaGaGqY"},"outputs":[],"source":["# Few Hyperparameters before we start things off\n","hidden_dim = 200\n","batch_size = 50\n","\n","epochs = 5\n","learning_rate = 0.00005\n","\n","logdir = './dropout_logs'\n","if os.path.exists(logdir):\n","    # We delete the logs on the first run\n","    shutil.rmtree(logdir)\n","os.mkdir(logdir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6dEsnyQaGqY"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir dropout_logs"]},{"cell_type":"markdown","metadata":{"id":"a1aw1aUlaGqZ"},"source":["<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p> As before, we have already implemented those two networks for you. You may check them out in <code>exercise_code/BatchNormModel.py</code>. As always, feel free to play around with the parameters here. Run the following two cells to setup both models and train them for a few epochs in order to compare the performance with and without Dropout. \n"," </p>\n","</div>\n","\n","### Train a model without Dropout\n","\n","Let us first start with a simple network `SimpleNetwork` which does not make use of Dropout."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkK2XC8QaGqZ"},"outputs":[],"source":["# train a model without Dropout\n","model = SimpleNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate)\n","simple_network_logger = TensorBoardLogger(\n","    save_dir=logdir,\n","    name='simple_network'\n",")\n","trainer = pl.Trainer(max_epochs=epochs, logger=simple_network_logger)\n","\n","trainer.fit(model)"]},{"cell_type":"markdown","metadata":{"id":"t4nhlwNkaGqZ"},"source":["### Train a model incl. Dropout\n","\n","Now that we have already seen how our simple network should work, let us look at the model `DropoutNetwork` that is actually using Dropout."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqOmVaiIaGqZ"},"outputs":[],"source":["# train a model with Dropout\n","model = DropoutNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate,dropout_p=0.75)\n","dropout_network_logger = TensorBoardLogger(\n","    save_dir=logdir,\n","    name='dropout_network'\n",")\n","trainer = pl.Trainer(max_epochs=epochs, logger=dropout_network_logger)\n","\n","trainer.fit(model)"]},{"cell_type":"markdown","metadata":{"id":"kSytjB5LaGqZ"},"source":["### Observations\n","\n","Take a look at TensorBoard to compare the performance of both networks:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CN6rr7n0aGqZ"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir dropout_logs"]},{"cell_type":"markdown","metadata":{"id":"RRakyfTDaGqZ"},"source":["As you can see, by using Dropout we can see that the Training Loss would increase but the model would perform better on the Validation Set. Like Batch Normalization, Dropout also has different behavior at Train and Test time. "]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Optional-BatchNormalization&Dropout.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}