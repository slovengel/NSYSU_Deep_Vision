{"cells":[{"cell_type":"markdown","metadata":{"id":"TIRRs51WXAIf"},"source":["# Autoencoder for MNIST in Pytorch Lightning\n","\n","In this notebook,you will train an autoencoder for the MNIST dataset, which is a dataset of handwritten digits. This is the last exercise where we will provide a structured skeleton. For future exercises, we will only provide the dataset, task as well as a test scenario for you to challenge yourself against your peers on our leaderboards.\n","\n","You will use the PyTorch Lightning framework which makes everything much more convenient! In case you haven't done yet, you should definitely check out the **PyTorch Lightning Introduction** in **Exercise 7**! However, you will mostly design the network architectures as well as be left of to choose suitable hyperparameters for the task at handy, so superficial knowledge of pytorch lightning will suffice.\n","\n","## What we will do:\n","\n","One application of autoencoders is unsupervised pretraining with unlabeled data and then finetuning the encoder with labeled data. This can increase our performance if there is only little labeled data but a lot of unlabeled data available.\n","\n","In this exercise you use the MNIST dataset with 60,000 images of handwritten digits, but you do not have all the labels available.\n","\n","You will then train our autoencoder to reproduce the unlabeled images. \n","\n","Then you will transfer the pretrained encoder weights and finetune a classifier on the labeled data for classifying the handwritten digits. This is called **transfer learning**."]},{"cell_type":"markdown","metadata":{"id":"37xkk9Y3XAIj"},"source":["**Note**: If you are running this in a google colab notebook, we recommend you enable GPU usage:\n","\n","> **Runtime** â€†â€†â†’â€†â€† **Change runtime type** â€†â€†â†’â€†â€† **Hardware Accelerator: GPU**\n","\n","If you are running in colab, you should install the dependencies by running the following cell:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jwyct5WjXAIk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650386181632,"user_tz":-480,"elapsed":10395,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"outputId":"d81a7a75-636a-469d-8535-751a50e7bcb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping pytorch-lightning as it is not installed.\u001b[0m\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[K     |████████████████████████████████| 582 kB 14.5 MB/s \n","\u001b[?25hCollecting PyYAML>=5.4\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 58.3 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.1.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 64.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.0)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.6)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n","\u001b[K     |████████████████████████████████| 408 kB 62.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 58.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.8)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 69.0 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 4.3 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 66.4 MB/s \n","\u001b[?25hInstalling collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 torchmetrics-0.8.0 yarl-1.7.2\n"]}],"source":["!pip uninstall pytorch-lightning\n","!pip install pytorch-lightning"]},{"cell_type":"markdown","metadata":{"id":"hEDWAZ7-ZA4E"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJCiVLV5o9QO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650386239016,"user_tz":-480,"elapsed":57390,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"outputId":"21fe8280-bfa9-4c40-d755-200243bc78fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","[Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks'\n","/content\n","['3.pytorch_lightning.ipynb', 'Optional-BatchNormalization_Dropout.ipynb', 'test.pt', 'training.pt', 'exercise08.zip', 'img', 'exercise_code', '1_Autoencoder_PyTorch_Lightning.ipynb']\n"]}],"source":["import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive \n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/Colab Notebooks\n","\n","os.listdir()\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'HW8/8' # Please change to your folder\n","GOOGLE_DRIVE_PATH = os.path.join('/content','drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","import torch\n","\n","from torchvision import transforms\n","import pytorch_lightning as pl\n","from exercise_code.image_folder_dataset import ImageFolderDataset\n","from pytorch_lightning.loggers import TensorBoardLogger\n","torch.manual_seed(42)\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"dvaj6myXS7nN"},"source":["<div class=\"alert alert-warning\">\n","    <h3>Note: Google Colab</h3>\n","    <p>\n","In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but, of course, you can also run this notebook on your CPU.\n","         </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWgm75NnS9hr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650386239017,"user_tz":-480,"elapsed":19,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"outputId":"366b99ae-920a-42f8-bd51-c8c102807847"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"Pm_rTAPnpsUo"},"source":["## Setup TensorBoard\n","In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations to your TensorBoard!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NV8ETXUQXAIo","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1650386299268,"user_tz":-480,"elapsed":60267,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"outputId":"51c0e3f5-50a3-4db9-9af5-3bc6a061f866"},"outputs":[{"output_type":"display_data","data":{"text/plain":["ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 333."]},"metadata":{}}],"source":["%load_ext tensorboard\n","%tensorboard --logdir lightning_logs --port 6005"]},{"cell_type":"markdown","metadata":{"id":"AgM1jfAcXAIp"},"source":["# 1. The MNIST Dataset\n","\n","First, you download the dataset. MNIST is a dataset of 60,000 images depicting handwritten digits. However, as with most datasets, labeling is a costly process and therefore we are left in a pickle.\n","\n","A good starting point is to label a small subset of your images. You either do this yourself but in this instance we consider the case where you hired another student to do it for you. After writing a labeling tool and some time, you are provided with 300 labeled images of which 100 will be used for training, 100 for validation, and 100 for testing. A problematic small number...\n","\n","Feel free to define some transforms now or later (you can also pass without any transforms).\n","\n","**Note**: We do **NOT** apply any transformations to test set at the time of final evaluation on the server."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"cvIxj8vuXAIp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650386307672,"user_tz":-480,"elapsed":8415,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"outputId":"30bd6705-81a3-414a-f2f9-60d828dd6df7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/HW8/8\n","['8', 'datasets']\n","/content/drive/My Drive/HW8\n","/content/drive/My Drive/HW8/8\n","Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n","'--force_download=True'\n","http://i2dl.vc.in.tum.de/static/data/mnist.zip\n","Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n","'--force_download=True'\n","http://i2dl.vc.in.tum.de/static/data/mnist.zip\n","Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n","'--force_download=True'\n","http://i2dl.vc.in.tum.de/static/data/mnist.zip\n","Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n","'--force_download=True'\n","http://i2dl.vc.in.tum.de/static/data/mnist.zip\n","Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n","'--force_download=True'\n","http://i2dl.vc.in.tum.de/static/data/mnist.zip\n"]}],"source":["########################################################################\n","# TODO: Feel free to define transforms                                 #\n","########################################################################\n","\n","transform = None\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################\n","\n","\n","i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n","mnist_root = os.path.join(i2dl_exercises_path, \"datasets\", \"mnist\")\n","\n","print(os.getcwd())\n","trainPt = torch.load(os.path.join(mnist_root, 'training.pt'))\n","testPt = torch.load(os.path.join(mnist_root, 'test.pt'))\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'HW8' # Please change to your folder\n","GOOGLE_DRIVE_PATH = os.path.join('/content','drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","print(os.getcwd())\n","\n","torch.save(trainPt[0][0:99],'datasets/mnist/train_images.pt')\n","torch.save(trainPt[0][100:199],'datasets/mnist/val_images.pt')\n","torch.save(trainPt[0][200:299],'datasets/mnist/test_images.pt')\n","torch.save(trainPt[1][0:99],'datasets/mnist/train_labels.pt')\n","torch.save(trainPt[1][100:199],'datasets/mnist/val_labels.pt')\n","torch.save(trainPt[1][200:299],'datasets/mnist/test_labels.pt')\n","torch.save(trainPt[0][300:399],'datasets/mnist/unlabeled_train_images.pt')\n","torch.save(trainPt[1][300:399],'datasets/mnist/unlabeled_val_images.pt')\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'HW8/8' # Please change to your folder\n","GOOGLE_DRIVE_PATH = os.path.join('/content','drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","\n","\n","i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n","mnist_root = os.path.join(i2dl_exercises_path, \"datasets\", \"mnist\")\n","print(os.getcwd())\n","\n","train = ImageFolderDataset(root=mnist_root,images='train_images.pt',labels='train_labels.pt',force_download=False,verbose=True,transform=transform)\n","val = ImageFolderDataset(root=mnist_root,images='val_images.pt',labels='val_labels.pt',force_download=False,verbose=True,transform=transform)\n","test = ImageFolderDataset(root=mnist_root,images='test_images.pt',labels='test_labels.pt',force_download=False,verbose=True,transform=transform)\n","\n","\n","# We also set up the unlabeled images which we will use later\n","unlabeled_train = ImageFolderDataset(root=mnist_root,images='unlabeled_train_images.pt',force_download=False,verbose=True,transform=transform)\n","unlabeled_val = ImageFolderDataset(root=mnist_root,images='unlabeled_val_images.pt',force_download=False,verbose=True,transform=transform)"]},{"cell_type":"markdown","metadata":{"id":"UhjeLIytXAIq"},"source":["The dataset consists of tuples of 28x28 pixel PIL images and a label that is an integer from 0 to 9. \n","\n","Let's turn a few of the images into numpy arrays, to look at their shape and visualize them and see if the labels we paid for are correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_aiSDRoXAIq","colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"status":"ok","timestamp":1650386308800,"user_tz":-480,"elapsed":1131,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"outputId":"cf6a9269-5eec-4b48-cda8-aea3130abea6"},"outputs":[{"output_type":"stream","name":"stdout","text":["The shape of our greyscale images:  (28, 28)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x432 with 9 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.rcParams['figure.figsize'] = (6,6) # Make the figures a bit bigger\n","\n","for i in range(9):\n","    image = np.array(train[i][0].squeeze()) # get the image of the data sample\n","    label = train[i][1] # get the label of the data sample\n","    plt.subplot(3,3,i+1)\n","    plt.imshow(image, cmap='gray', interpolation='none')\n","    plt.title(\"Class {}\".format(label))\n","    \n","plt.tight_layout()\n","print('The shape of our greyscale images: ', image.shape)"]},{"cell_type":"markdown","metadata":{"id":"V9sz_lHyqJoj"},"source":["<div class=\"alert alert-warning\">\n","    <h3>Note: Starting Simple</h3>\n","    <p>\n","Regardless of the size of our dataset, the first step we have to take is to evaluate the performance of a simple classifier. Always approach a problem with a simple approach first and go from there to see which changes are helping you.\n","         </p>\n","</div>\n","\n","# 2. A Simple Classifier\n","\n","In `exercise_code/models.py` we prepared all classes for you which you will finalize throughout the notebook to build an Autoencoder and an image classifier with Pytorch Lightning. If you are unfamiliar with Pytorch Lightning, you can check out the notebook from the last exercise.\n","\n","![network_split](img/network_split.png)\n","\n","## 2.1 The Encoder\n","\n","Different to previous models, we are going to split up our model into two parts: the so called `encoder` and the `classifier`. The `classifier` has a static task as it will output our predictions given a one-dimensional input. The `encoder`'s task is to extract meaningful information out of our input so that the classifier can make a proper decision. Right now however, both networks will be consisting of linear layers coupled with auxiliary ones and therefore won't be too different in their design. This split-up will be relevant later, e.g., by using convolutional layers which are introduced in the lecture. We are going to set up the `encoder` now. \n","\n","Think about a good network architecture. You're completely free here and can come up with any network you like! (\\*)\n","\n","Have a look at the documentation of `torch.nn` at https://pytorch.org/docs/stable/nn.html to learn how to use this module to build your network!\n","\n","Then implement your architecture: initialize it in `__init__()` and assign it to `self.model`. This is particularly easy using `nn.Sequential()` which you only have to pass the list of your layers. \n","\n","To make your model customizable and support parameter search, don't use hardcoded hyperparameters - instead, pass them as dictionary `hparams` (here, `n_hidden` is the number of neurons in the hidden layer) when initializing `models`.\n","\n","Here's an simple example:\n","\n","```python\n","        self.model = nn.Sequential(\n","            nn.Linear(input_size, self.hparams[\"n_hidden\"]),\n","            nn.ReLU(),            \n","            nn.Linear(self.hparams[\"n_hidden\"], num_classes)\n","        )\n","```\n","\n","Have a look at the forward path in `forward(self, x)`, which is so easy that you don't need to implement it yourself.\n","\n","As PyTorch automatically computes the gradients, that's all we need to do! No need to manually calculate derivatives for the backward paths anymore! :)\n","\n","\n","____\n","\\* *The size of your final model must be less than 20 MB, which is approximately equivalent to 5 Mio. params. Note that this limit is quite lenient, you will probably need much less parameters!*\n","\n","*In order to keep things simple, you should only use fully connected layers for this task as we need to revert the encoder architecture  later on in the notebook.*\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Implement the <code>Encoder</code> class initialization in <code>exercise_code/models.py</code>.\n","    </p>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"bOYbUg8lAmgU"},"source":["## 2.2 The Classifier\n","\n","Now we are implementing our classifier. It will use the encoder network that you have defined in the above cell. By looking at `Classifier.forward`, you can see that we are simply chaining the `classifier` as well as the `encoder` together. Therefore, you have to match the input shape of the classifier to the output shape of your encoder implemented above. \n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Implement the <code>Classifier</code> class network initialization in <code>exercise_code/models.py</code>.\n","    </p>\n","</div>\n","\n","## 2.3 Training & Validation Step\n","\n","In pytorch lightning, you only have to provide a training step and a validation step. We implemented both for you, but please check the codes out to make sure that you understand them.\n","\n","\n","<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>Have a look at the functions <code>training_step</code> and <code>validation_step</code> of the <code>Classifier</code> class in <code>exercise_code/models.py</code>, that take a batch as input and calculate the loss. \n"," </p>\n","</div>\n","\n","## 2.4 Optimizer\n","Lastly, implement the function `configure_optimizers()` to define your optimizer. Here the documentation of `torch.optim`at https://pytorch.org/docs/stable/optim.html might be helpful.\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Implement the <code>configure_optimizers</code> method of the <code>Classifier</code> in <code>exercise_code/models.py</code>.\n","    </p>\n","</div>\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"s7BAx0AZXAIs"},"outputs":[],"source":["from exercise_code.models import Encoder\n","from exercise_code.models import Classifier\n","\n","########################################################################\n","# TODO: Define your hyper parameters here!                             #\n","########################################################################\n","\n","hparams = {'n_hidden':10, 'batch_size':10, 'layer_1_dim':128}\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################\n","\n","encoder = Encoder(hparams)\n","classifier = Classifier(hparams, encoder, train, val, test)"]},{"cell_type":"markdown","metadata":{"id":"iopuWivWXAIs"},"source":["## 2.5 Fit Classification Model with Trainer\n","Now it's time to train your model.\n","\n","Have a look of the documentation of `pl.Trainer` at https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html to find out which arguments you can pass to define your training process.\n","\n","Then, you can start the training with `trainer.fit(classifier)` and have a look at the loss and the training accuracy in TensorBoard."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"i5id52-HXAIs","colab":{"base_uri":"https://localhost:8080/","height":377,"referenced_widgets":["a05345efbabc4aa3b9e886c5ad379d17","4075b49402ea47f19680b6f667bc50bc","381defc35f204a7cbb87f5559745e965","d08927614208447999cc16aed3832d6c","509019870607483da0869bc4bab7ab07","7df234bbd94e4084aeb9ad8ff0740906","7b39fb1ca5c7471b87c0c7248fc8e194","8762d8c924ba49c496e1172b6ab33573","074b5a9618224eba837cc16a73d06c4a","4ec675ae11c54f43a032e8857ee21d2a","be1f5dc70a28484d8dbeebd34e9f85c5"]},"executionInfo":{"status":"ok","timestamp":1650386320685,"user_tz":-480,"elapsed":10424,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"outputId":"502c303e-b094-43b9-87b3-a9d006f7d74d"},"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","Missing logger folder: /content/drive/My Drive/HW8/8/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type     | Params\n","-------------------------------------\n","0 | encoder | Encoder  | 16.5 K\n","1 | model   | Identity | 0     \n","-------------------------------------\n","16.5 K    Trainable params\n","0         Non-trainable params\n","16.5 K    Total params\n","0.066     Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05345efbabc4aa3b9e886c5ad379d17"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["tensor([17, -2, 30, 21, 35, 17,  3, 11, -5, 11])\n","Validation accuracy when training from scratch: 75%\n"]}],"source":["import copy\n","trainer = None\n","\n","trainer = pl.Trainer(\n","    max_epochs=100,\n","    gpus=1 if torch.cuda.is_available() else None\n",")\n","\n","trainer.fit(classifier) # train the standard classifier\n","print(\"Validation accuracy when training from scratch: {}%\".format(classifier.getAcc(classifier.val_dataloader())[1]*100))"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"_WWoQpMQXAIs"},"source":["# 3. Autoencoder\n","\n","One hundred images as training data are not much. How could we improve our performance with limited data? We have no money left to pay our student for more labels, and labeling the data ourselves is out of question. A good idea would be to do data augmentation to get the most out of our few labeled instances, but here we provide another way to solve this problem: we will use our large amount of unlabeled data to do unsupervised pretraining with an autoencoder, and then transfer the weights of our encoder to our classifier.\n","\n","For each image input, the autoencoder just tries to reproduce the same image as output. The difficulty behind is that the autoencoder has to go through a low dimensional bottleneck, which we call the **latent space**.\n","In other words, the autoencoder should learn to represent all the input information in the low dimensional latent space; it learns to compress the input distribution.\n","To make our model learn to reproduce the input, we use the mean squared error between our input pixels and the\n","output pixels as the loss function. For this loss we do not need any labels!\n","\n","![autoencoder](img/autoencoder.png)\n","\n","After this, our encoder has learned to extract meaningful information from the inputs. We can then transfer its weights\n","to a classifier architecture and finetune it with our labeled data, i.e., instead of initializing our encoder randomly we are re-using the weights of our trained encoder from our autoencoder network. This process is called **transfer learning**.\n","\n","![autoencoder_pretrained](img/pretrained.png)\n","\n","## 3.1 Decoder\n","\n","Before we can train our autoencoder, you have to initialize the your `decoder` architecture. The simplest way is to mirror your encoder architecture which ensure that the `latent space` output of our `encoder` is correctly transformed to our input shape.\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Implement the <code>Decoder</code> and <code>Autoencoder</code> class initialization in <code>exercise_code/models.py</code>.\n","    </p>\n","</div>\n","\n","## 3.2 Autoencoder Training\n","\n","Now, we can train the full autoencoder consisting of both en- and decoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xi4Y8lpcXAIt"},"outputs":[],"source":["from exercise_code.models import Autoencoder, Encoder, Decoder\n","\n","########################################################################\n","# TODO: Define your hyperparameters here!                              #\n","########################################################################\n","\n","hparams = {'n_hidden':10, 'batch_size':10, 'layer_1_dim':128}\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################\n","\n","encoder_pretrained = Encoder(hparams)\n","decoder = Decoder(hparams)\n","ae_logger = TensorBoardLogger(save_dir='lightning_logs')\n","autoencoder = Autoencoder(hparams, encoder_pretrained, decoder, unlabeled_train, unlabeled_val, ae_logger)"]},{"cell_type":"markdown","metadata":{"id":"yEz476R4XAIt"},"source":["Some tests to check whether we'll accept your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uwu1VC4xXAIt","executionInfo":{"status":"ok","timestamp":1650386321613,"user_tz":-480,"elapsed":933,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"af82e3b3-5e66-4bcc-e906-dbdbb553844b"},"outputs":[{"output_type":"stream","name":"stdout","text":["FYI: Your model has 0.024 mio. params.\n","Model accepted!\n"]}],"source":["from exercise_code.Util import printModelInfo, load_model\n","_ = printModelInfo(autoencoder)"]},{"cell_type":"markdown","metadata":{"id":"plQwnphtqggl"},"source":["Next, we define another trainer to fit our autoencoder. Keep in mind that an epoch here will take much longer since\n","we are iterating through 5,8600 images instead of just 100."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uuzXMq6zjbb","tags":[],"executionInfo":{"status":"ok","timestamp":1650386322566,"user_tz":-480,"elapsed":956,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"041e753a-0cc9-48b2-a725-413139559e01"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","  category=PossibleUserWarning,\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","Missing logger folder: /content/drive/My Drive/HW8/8/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]}],"source":["ae_trainer = None\n","\n","########################################################################\n","# TODO: Define your trainer! Don't forget the logger.                  #\n","########################################################################\n","\n","\n","ae_trainer = pl.Trainer(gpus=1)\n","\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################\n","ae_trainer.fit(autoencoder)"]},{"cell_type":"markdown","metadata":{"id":"4drAZCqAXAIu"},"source":["Once trained, let's have a look at the reconstructed validation images (If you have not already looked at them in TensorBoard)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23GTLKPbXAIu","executionInfo":{"status":"ok","timestamp":1650386494744,"user_tz":-480,"elapsed":1534,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"colab":{"base_uri":"https://localhost:8080/","height":441},"outputId":"f288f26a-e9db-4855-8916-5eae06211c49"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x432 with 64 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["reconstructions = autoencoder.getReconstructions()\n","for i in range(64):\n","    plt.subplot(8,8,i+1)\n","    plt.axis('off')\n","    plt.imshow(reconstructions[i], cmap='gray', interpolation='none')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"rL4x_b8fXAIu"},"source":["# 4. Transfer Learning\n","\n","## 4.1 The pretrained Classifier\n","\n","Now we initialize another classifier but this time with the pretrained encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hy8cpa9XAIv"},"outputs":[],"source":["from exercise_code.models import Classifier\n","\n","########################################################################\n","# TODO: Define your hyper parameters here!                             #\n","########################################################################\n","\n","hparams = {}\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################\n","classifier_pretrained = Classifier(hparams, encoder_pretrained, train, val, test)"]},{"cell_type":"markdown","metadata":{"id":"kLteX-dgXAIv"},"source":["Now specify another trainer that we will use the pretrained classifier to compare its performance with\n","the classifier we trained on only the labeled data. You might need to optimize the parameters defined above in order to achieve a reasonable result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxtDkPJNXAIv","executionInfo":{"status":"ok","timestamp":1650386337308,"user_tz":-480,"elapsed":573,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"colab":{"base_uri":"https://localhost:8080/","height":416,"referenced_widgets":["eaa0685336714cb1b5aad7cb61449f56","efb2b06214824ca39a8a1c3e68c05954","2026669b0ca847efbe453f9063045378","2e4f534c524d4fd0981f342decef2e1e","68c650c3bb3949b7bdfe46e7c3406fed","3da90921f238495ab5f73dfba6654a36","b9d4476d66af406ea64c34e92f947394","76f55f3b957c49ada461758bc9f23423","7b25775527334ceaafd27071faa6f021","10e764b8379e4d01a96af32fb0f8f12d","8cb3243a2b474cff8f608adf45f60b4e"]},"outputId":"013aa4e0-0fb1-4950-f55b-0f19bb3e0711"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","  category=PossibleUserWarning,\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","Missing logger folder: /content/drive/My Drive/HW8/8/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type     | Params\n","-------------------------------------\n","0 | encoder | Encoder  | 16.5 K\n","1 | model   | Identity | 0     \n","-------------------------------------\n","16.5 K    Trainable params\n","0         Non-trainable params\n","16.5 K    Total params\n","0.066     Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaa0685336714cb1b5aad7cb61449f56"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}],"source":["trainer = None\n","\n","########################################################################\n","# TODO: Define your trainer! Don't forget the logger.                  # \n","# Hint: Choose an appropriate logging frequency in your trainer.       #\n","########################################################################\n","\n","trainer = pl.Trainer(gpus=1)\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################\n","\n","trainer.fit(classifier_pretrained) # train the standard classifier"]},{"cell_type":"markdown","metadata":{"id":"c2WRtGnqXAIv"},"source":["Let's have a look at the validation accuracy of the two different classifiers and compare them. And don't forget that you can also monitor your training in TensorBoard.\n","\n","We will only look at the test accuracy and compare our two classifiers with respect to that in the very end."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXTED0-rXAIv","executionInfo":{"status":"ok","timestamp":1650386340988,"user_tz":-480,"elapsed":3,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5d24cc22-1db5-477a-d301-0b642fdabcc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation accuracy when training from scratch: 100%\n","Validation accuracy with pretraining: 68%\n"]}],"source":["print(\"Validation accuracy when training from scratch: {}%\".format(classifier.getAcc(classifier.val_dataloader())[1]*100))\n","print(\"Validation accuracy with pretraining: {}%\".format(classifier_pretrained.getAcc(classifier.val_dataloader())[1]*100))"]},{"cell_type":"markdown","metadata":{"id":"zAp2OTyf4_5b"},"source":["Now that everything is working, feel free to play around with different architectures. As you've seen, it's really easy to define your model or do changes there.\n","\n","To pass this submission, you'll need an accuracy of **55%**."]},{"cell_type":"markdown","metadata":{"id":"OmEYmRT-5S-e"},"source":["# Save your model & Report Test Accuracy\n","\n","When you've done with your **hyperparameter tuning**, have achieved **at least 55% validation accuracy** and are happy with your final model, you can save it here.\n","\n","Before that, please check again whether the number of parameters is below 5 Mio and the file size is below 20 MB.\n","\n","When your final model is saved, we'll lastly report the test accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S69ETKxD5TcE","executionInfo":{"status":"ok","timestamp":1650386343885,"user_tz":-480,"elapsed":351,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66f835da-c1b0-4ac1-d900-a0edeb2b755a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy when training from scratch: 55%\n"]}],"source":["from exercise_code.Util import test_and_save\n","\n","print(\"Test accuracy when training from scratch: {}%\".format(classifier.getAcc()[1]*100))\n","print('\\nNow to the pretrained classifier:')\n","test_and_save(classifier_pretrained)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QoKfCe0GXAIw","executionInfo":{"status":"ok","timestamp":1650386353460,"user_tz":-480,"elapsed":8166,"user":{"displayName":"郭晏涵","userId":"05346489510425247454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a992507-8228-4958-864b-ff23bfd7fc5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["relevant folders: ['exercise_code']\n","notebooks files: ['3.pytorch_lightning.ipynb', 'Optional-BatchNormalization_Dropout.ipynb', '1_Autoencoder_PyTorch_Lightning.ipynb']\n","Adding folder exercise_code\n","Adding notebook 3.pytorch_lightning.ipynb\n","Adding notebook Optional-BatchNormalization_Dropout.ipynb\n","Adding notebook 1_Autoencoder_PyTorch_Lightning.ipynb\n","Zipping successful! Zip is stored under: /content/drive/My Drive/HW8/8/exercise08.zip\n"]}],"source":["# Now zip the folder for upload\n","from exercise_code.submit import submit_exercise\n","\n","submit_exercise('exercise08')"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"zmuAlwhmXAIw"},"source":["Congrats! You've now finished your first autoencoder and transferred the weights to a classifier! Much easier than in plain numpy, right? But wait, to complete the exercise\n","\n","# Submission Goals\n","\n","- Goal: Successfully implement a fully connected autoencoder for MNIST with Pytorch Lightning and transfer the encoder weights to a classifier.\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"1_Autoencoder_PyTorch_Lightning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a05345efbabc4aa3b9e886c5ad379d17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4075b49402ea47f19680b6f667bc50bc","IPY_MODEL_381defc35f204a7cbb87f5559745e965","IPY_MODEL_d08927614208447999cc16aed3832d6c"],"layout":"IPY_MODEL_509019870607483da0869bc4bab7ab07"}},"4075b49402ea47f19680b6f667bc50bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7df234bbd94e4084aeb9ad8ff0740906","placeholder":"​","style":"IPY_MODEL_7b39fb1ca5c7471b87c0c7248fc8e194","value":"Sanity Checking DataLoader 0:   0%"}},"381defc35f204a7cbb87f5559745e965":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_8762d8c924ba49c496e1172b6ab33573","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_074b5a9618224eba837cc16a73d06c4a","value":0}},"d08927614208447999cc16aed3832d6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ec675ae11c54f43a032e8857ee21d2a","placeholder":"​","style":"IPY_MODEL_be1f5dc70a28484d8dbeebd34e9f85c5","value":" 0/2 [02:53&lt;?, ?it/s]"}},"509019870607483da0869bc4bab7ab07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"7df234bbd94e4084aeb9ad8ff0740906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b39fb1ca5c7471b87c0c7248fc8e194":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8762d8c924ba49c496e1172b6ab33573":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"074b5a9618224eba837cc16a73d06c4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ec675ae11c54f43a032e8857ee21d2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be1f5dc70a28484d8dbeebd34e9f85c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eaa0685336714cb1b5aad7cb61449f56":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efb2b06214824ca39a8a1c3e68c05954","IPY_MODEL_2026669b0ca847efbe453f9063045378","IPY_MODEL_2e4f534c524d4fd0981f342decef2e1e"],"layout":"IPY_MODEL_68c650c3bb3949b7bdfe46e7c3406fed"}},"efb2b06214824ca39a8a1c3e68c05954":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3da90921f238495ab5f73dfba6654a36","placeholder":"​","style":"IPY_MODEL_b9d4476d66af406ea64c34e92f947394","value":"Sanity Checking: "}},"2026669b0ca847efbe453f9063045378":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_76f55f3b957c49ada461758bc9f23423","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b25775527334ceaafd27071faa6f021","value":0}},"2e4f534c524d4fd0981f342decef2e1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10e764b8379e4d01a96af32fb0f8f12d","placeholder":"​","style":"IPY_MODEL_8cb3243a2b474cff8f608adf45f60b4e","value":" 0/? [00:00&lt;?, ?it/s]"}},"68c650c3bb3949b7bdfe46e7c3406fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"3da90921f238495ab5f73dfba6654a36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9d4476d66af406ea64c34e92f947394":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76f55f3b957c49ada461758bc9f23423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b25775527334ceaafd27071faa6f021":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10e764b8379e4d01a96af32fb0f8f12d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cb3243a2b474cff8f608adf45f60b4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}